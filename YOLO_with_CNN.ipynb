{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YJI1iwI8UBWz"
   },
   "source": [
    "# Downloading and importing requiered libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "V3jv5cMURL0H",
    "outputId": "f98adbb9-dee6-4be4-a4ac-c822028f4b7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All requiered modules have been downloaded\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "!git clone https://github.com/sodeeplearning/simpletorch\n",
    "!pip install ultralytics\n",
    "\n",
    "clear_output(True)\n",
    "print(\"All requiered modules have been downloaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "KYyFArKgU9Gb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All requiered libraries have been imported\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import simpletorch as ST\n",
    "import io\n",
    "import zipfile\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torchsummary import summary\n",
    "import matplotlib.pyplot as plt\n",
    "import supervision as sv\n",
    "import torchvision\n",
    "\n",
    "print(\"All requiered libraries have been imported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "WSgwj5OmVFyT"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sZ7MpyY1Wcwg"
   },
   "source": [
    "# Downloading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DbSig3UGWfpK"
   },
   "outputs": [],
   "source": [
    "print(\"Downloading has been started\")\n",
    "\n",
    "url = \"https://storage.yandexcloud.net/net-nomer-dataset/Net-Nomer-a-data_processing.zip\"\n",
    "\n",
    "response = requests.get(url)\n",
    "zip = zipfile.ZipFile(io.BytesIO(response.content))\n",
    "zip.extractall()\n",
    "del zip\n",
    "\n",
    "print(\"The dataset has been downloaded\")\n",
    "print(\"Starting detection dataset processing\")\n",
    "\n",
    "def image_num(path):\n",
    "  last_slash = path.rfind('/')\n",
    "  dot = path.rfind('.')\n",
    "  return int(path[last_slash + 1:dot])\n",
    "\n",
    "def string_perf(string):\n",
    "    return int(string[string.find('>') + 1: string.rfind('<')])\n",
    "\n",
    "def xml_perf(path):\n",
    "    with open(path, 'r') as f:\n",
    "        file = f.read()\n",
    "        massive = file.split('\\n')\n",
    "        return (string_perf(massive[-7]) / 2592,\n",
    "                string_perf(massive[-6]) / 1552,\n",
    "                string_perf(massive[-5]) / 2592,\n",
    "                string_perf(massive[-4]) / 1552)\n",
    "\n",
    "def bboxes_dataset(path_to_folder):\n",
    "  files = sorted(ST.getting_files(path_to_folder), key=image_num)\n",
    "  answer_tensor = torch.zeros((len(files), 4))\n",
    "  for ind, current_path in enumerate(files):\n",
    "    answer_tensor[ind] = torch.tensor(xml_perf(current_path))\n",
    "  print(files)\n",
    "  return answer_tensor\n",
    "\n",
    "train_bboxes_dataset = bboxes_dataset(\"Dataset/boxes\")\n",
    "\n",
    "print(\"The dataset has been performed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hnGCgyu9XATN"
   },
   "source": [
    "# Getting model and prediction analizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D5Mto47XXB9n"
   },
   "outputs": [],
   "source": [
    "detection_model = YOLO('yolov9c.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e-_0vryLXGsg"
   },
   "outputs": [],
   "source": [
    "output = detection_model(\"Dataset/photo/0.jpg\")\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PhXahyDhabHO"
   },
   "source": [
    "# Adaptating dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JxqkLuXr9MQT"
   },
   "source": [
    "## Getting dataset (Skip if loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q3Qu05MTecAK"
   },
   "outputs": [],
   "source": [
    "multy_factor = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q5JlSHgHaFNO"
   },
   "outputs": [],
   "source": [
    "def cropping_image(image, x_min, y_min, x_max, y_max, x_shape, y_shape):\n",
    "  top = int(y_min * y_shape)\n",
    "  left = int(x_min * x_shape)\n",
    "  height = int((y_max - y_min) * y_shape)\n",
    "  width = int((x_max - x_min) * x_shape)\n",
    "  return transforms.functional.crop(img = image,\n",
    "                                    top = top,\n",
    "                                    left = left,\n",
    "                                    height = height,\n",
    "                                    width = width)\n",
    "\n",
    "def detection_pred_processing(image_id):\n",
    "  image_path = f\"Dataset/photo/{image_id}.jpg\"\n",
    "  detection_output = detection_model(image_path)[0]\n",
    "  answer_tensor = torch.zeros((0, 4))\n",
    "  number_plate_coord = train_bboxes_dataset[image_id]\n",
    "  nx_min, ny_min, nx_max, ny_max = number_plate_coord\n",
    "  classes_with_number = [2, 3, 5, 7]\n",
    "  transform = transforms.Resize((512, 512))\n",
    "  answer_images = torch.zeros((0, 3, 512, 512))\n",
    "  image_tensor = ST.jpg_tensor(image_path)\n",
    "  y_shape, x_shape = image_tensor.shape[-2:]\n",
    "\n",
    "  for current_class, (x_min, y_min, x_max, y_max) in zip(detection_output.boxes.cls, detection_output.boxes.xyxyn):\n",
    "    if current_class.item() in classes_with_number:\n",
    "      if x_min < nx_min and y_min < ny_min and x_max > nx_max and y_max > ny_max:\n",
    "        coordinates = torch.tensor([(nx_min - x_min) / (x_max - x_min) * 0.96,\n",
    "                                    (ny_min - y_min) / (y_max - y_min) * 0.97,\n",
    "                                    (nx_max - x_min) / (x_max - x_min) * 1.04,\n",
    "                                    (ny_max - y_min) / (y_max - y_min) * 1.03]).unsqueeze(0)\n",
    "        answer_tensor = torch.cat((answer_tensor,\n",
    "                                   coordinates),dim=0)\n",
    "        adding_image = transform(cropping_image(image = image_tensor,\n",
    "                                      x_min = x_min,\n",
    "                                      y_min = y_min,\n",
    "                                      x_max = x_max,\n",
    "                                      y_max = y_max,\n",
    "                                      x_shape = x_shape,\n",
    "                                      y_shape = y_shape)).unsqueeze(0)\n",
    "        answer_images = torch.cat((answer_images,\n",
    "                                   adding_image), dim=0)\n",
    "\n",
    "  return answer_images, answer_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MQ3q6ApCm_2D"
   },
   "outputs": [],
   "source": [
    "def get_dataset():\n",
    "  answer_images = torch.zeros((0, 3, 512, 512))\n",
    "  answer_bboxes = torch.zeros((0, 4))\n",
    "\n",
    "  for i in range(202):\n",
    "    print(f\"{i + 1} sample now\")\n",
    "    current_images, current_bboxes = detection_pred_processing(i)\n",
    "    answer_images = torch.cat((answer_images,\n",
    "                               current_images), dim=0)\n",
    "    answer_bboxes = torch.cat((answer_bboxes,\n",
    "                               current_bboxes), dim=0)\n",
    "\n",
    "  return answer_images, answer_bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S8IUN6oBwreS"
   },
   "outputs": [],
   "source": [
    "images_dataset, bboxes_dataset = get_dataset()\n",
    "clear_output(True)\n",
    "print(\"The dataset has been performed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jSnql2_K5oQV"
   },
   "outputs": [],
   "source": [
    "def multy_image(image, multy_factor): # Increasing dataset size in multy_factor times\n",
    "    augmentation = ST.transforms.Compose([\n",
    "        ST.transforms.ToPILImage(),\n",
    "        ST.transforms.ColorJitter(\n",
    "            brightness=0.4,\n",
    "            contrast=0.3,\n",
    "            saturation=0.3,\n",
    "            hue=0.1,\n",
    "        ),\n",
    "        ST.transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    answer = torch.zeros((multy_factor, 3, 512, 512))\n",
    "\n",
    "    for current_augmentation in range(multy_factor):\n",
    "        answer[current_augmentation] = augmentation(image)\n",
    "\n",
    "    return answer\n",
    "\n",
    "def augment_dataset(dataset, multy_factor, num_of_images = 202):\n",
    "    images_tensor = dataset\n",
    "\n",
    "    train_images = torch.zeros((0, 3, 512, 512))\n",
    "    for ind, current_image in enumerate(images_tensor):\n",
    "        train_images = torch.cat([train_images, multy_image(\n",
    "            image = current_image,\n",
    "            multy_factor = multy_factor\n",
    "        )], dim = 0)\n",
    "    return train_images\n",
    "\n",
    "augmented_images_dataset = augment_dataset(dataset = images_dataset,\n",
    "                                           multy_factor = multy_factor,\n",
    "                                           num_of_images = images_dataset.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z6WfcOr-7sCW"
   },
   "outputs": [],
   "source": [
    "saving_dir = \"drive/MyDrive/\"\n",
    "torch.save(augmented_images_dataset, saving_dir + \"Detection_images.pt\")\n",
    "torch.save(bboxes_dataset, saving_dir + \"Detection_bboxes.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZnDTrHSH9Cu8"
   },
   "source": [
    "##loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3shKpKCKeqcg"
   },
   "outputs": [],
   "source": [
    "multy_factor = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FYJ_sIGS80vL"
   },
   "outputs": [],
   "source": [
    "path_to_data = \"drive/MyDrive/\"\n",
    "augmented_images_dataset = torch.load(path_to_data + \"Detection_images.pt\")\n",
    "bboxes_dataset = torch.load(path_to_data + \"Detection_bboxes.pt\") * 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8D-Fo2GE-UEQ"
   },
   "outputs": [],
   "source": [
    "train_images = augmented_images_dataset[:160 * multy_factor]\n",
    "val_images = augmented_images_dataset[160 * multy_factor:]\n",
    "train_bboxes = bboxes_dataset[:160]\n",
    "val_bboxes = bboxes_dataset[160:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Va46AZA3-8Vt"
   },
   "source": [
    "#Getting CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ADIkNXB6_BDc"
   },
   "outputs": [],
   "source": [
    "class CNN_Model (torch.nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "\n",
    "    self.convolutions = torch.nn.Sequential(*[\n",
    "        ST.Conv_Block(\n",
    "            input_channels = 3,\n",
    "            output_channels = 128,\n",
    "        ),\n",
    "        ST.Conv_Block(\n",
    "            input_channels = 128,\n",
    "            output_channels = 128,\n",
    "        ),\n",
    "        ST.Conv_Block(\n",
    "            input_channels = 128,\n",
    "            output_channels = 256,\n",
    "        ),\n",
    "        ST.Conv_Block(\n",
    "            input_channels = 256,\n",
    "            output_channels = 512,\n",
    "        ),\n",
    "        ST.Conv_Block(\n",
    "            input_channels = 512,\n",
    "            output_channels = 512,\n",
    "        ),\n",
    "        ST.Conv_Block(\n",
    "            input_channels = 512,\n",
    "            output_channels = 512,\n",
    "        ),\n",
    "        torch.nn.Flatten()\n",
    "    ])\n",
    "\n",
    "    self.fs = torch.nn.Sequential(*[\n",
    "        torch.nn.Linear(32768, 100),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(100, 4),\n",
    "    ])\n",
    "\n",
    "  def forward(self, input_tensor):\n",
    "    feature_map = self.convolutions(input_tensor)\n",
    "    fs_bboxes = self.fs(feature_map)\n",
    "    return fs_bboxes\n",
    "\n",
    "cnn_model = CNN_Model().to(device)\n",
    "print(f\"{sum(p.numel() for p in cnn_model.parameters() if p.requires_grad)} params in model\")\n",
    "cnn_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u_ZQ9atNU4Z7"
   },
   "source": [
    "# Training CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vW_ENMdZdvoG"
   },
   "outputs": [],
   "source": [
    "def dist_loss(bboxes_pred, bboxes_true):\n",
    "  return torchvision.ops.distance_box_iou_loss(bboxes_pred, bboxes_true, reduction = 'mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d77lzwC3Jp6E"
   },
   "outputs": [],
   "source": [
    "def iou (bboxes_pred, bboxes_true):\n",
    "  return torchvision.ops.complete_box_iou_loss(train_pred, train_true, reduction = 'mean').log() * -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "26ZDWp8csGuI"
   },
   "outputs": [],
   "source": [
    "num_of_epochs = 2000\n",
    "train_batch_size = 15\n",
    "learning_rate = 1e-4\n",
    "scheduler_gamma = 0.5\n",
    "scheduler_freq = 400\n",
    "change_loss = 200\n",
    "\n",
    "show_every = 10\n",
    "val_every = 10\n",
    "val_batch_size = 15\n",
    "best_val = 0.1\n",
    "\n",
    "cnn_loss = torch.nn.L1Loss()\n",
    "optimizer = torch.optim.Adam(cnn_model.parameters(), lr = learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma = scheduler_gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 692
    },
    "id": "Bi1Xh_kMsK_U",
    "outputId": "f429789a-542a-4157-ade7-0411cb5a1418"
   },
   "outputs": [],
   "source": [
    "losses = {\"train\" : [], \"val\" : []}\n",
    "cnn_model.train()\n",
    "\n",
    "for epoch in range(1, num_of_epochs + 1):\n",
    "  optimizer.zero_grad()\n",
    "  train_batch = torch.randint(high = train_images.shape[0], size = [train_batch_size])\n",
    "\n",
    "  train_pred = cnn_model(train_images[train_batch].to(device))\n",
    "  train_true = train_bboxes[train_batch // multy_factor].to(device)\n",
    "\n",
    "  train_loss = cnn_loss(train_pred, train_true)# + dist_loss(train_pred, train_true)\n",
    "\n",
    "  train_loss.backward()\n",
    "  optimizer.step()\n",
    "\n",
    "  losses[\"train\"].append(train_loss.item())\n",
    "\n",
    "  if epoch % val_every == 0:\n",
    "    with torch.no_grad():\n",
    "      val_batch = torch.randint(high = val_images.shape[0], size = [val_batch_size])\n",
    "      val_pred = cnn_model(val_images[val_batch].to(device))\n",
    "      val_true = val_bboxes[val_batch // multy_factor].to(device)\n",
    "\n",
    "      val_loss = cnn_loss(val_pred, val_true)# + dist_loss(train_pred, train_true)\n",
    "\n",
    "      losses[\"val\"].append(val_loss.item())\n",
    "      print(val_loss.item())\n",
    "      if (val_loss.item() < best_val):\n",
    "        torch.save(cnn_model.state_dict(), \"drive/MyDrive/cnn_best_weights.pt\")\n",
    "        best_val = val_loss.item()\n",
    "        print(\"The weights have been updated\")\n",
    "\n",
    "  if epoch % scheduler_freq == 0:\n",
    "    scheduler.step()\n",
    "\n",
    "  if epoch % show_every == 0:\n",
    "      clear_output(True)\n",
    "      fig, ax = plt.subplots(figsize=(30, 10))\n",
    "      plt.title(\"Loss graph\")\n",
    "      plt.plot(losses[\"train\"], \".-\", label=\"Training Loss\")\n",
    "      plt.plot(torch.arange(0, epoch, show_every), losses[\"val\"], \".-\", label=\"Validation Loss\")\n",
    "      plt.xlabel(\"Iteration\")\n",
    "      plt.ylabel(\"Loss\")\n",
    "      plt.legend()\n",
    "      plt.grid()\n",
    "      plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g0rUtiChk0ys"
   },
   "source": [
    "# Checking CNN training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 435
    },
    "id": "JkxFs71i0e6o",
    "outputId": "364d3984-6508-45ea-99eb-6e863cf8ddef"
   },
   "outputs": [],
   "source": [
    "def cnn_pred(image_id, color_pred = 'blue', color_right = 'green'):\n",
    "  #cnn_model.load_state_dict(torch.load(\"best_weights.pt\"))\n",
    "  image = val_images[image_id]\n",
    "  #image = adjust_image(val_images[image_id])\n",
    "  cnn_model.train()\n",
    "  annotator = sv.BoxAnnotator()\n",
    "  transform = transforms.Resize((512, 512))\n",
    "  to_pil = transforms.ToPILImage()\n",
    "\n",
    "  image_tensor = transform(image.unsqueeze(0)).to(device)\n",
    "  coordinates = (cnn_model(image_tensor)).tolist()\n",
    "  rx0, ry0, rx1, ry1 = val_bboxes[image_id // multy_factor].clone()\n",
    "\n",
    "  x0, y0, x1, y1 = coordinates[0]\n",
    "  x_min = min(x0, x1)\n",
    "  x_max = max(x0, x1)\n",
    "  y_min = min(y0, y1)\n",
    "  y_max = max(y0, y1)\n",
    "  ST.imshow(image)\n",
    "  plt.vlines(x_min, y_min, y_max, color=color_pred)\n",
    "  plt.vlines(x_max, y_max, y_min, color=color_pred)\n",
    "  plt.hlines(y_min, x_min, x_max, color=color_pred)\n",
    "  plt.hlines(y_max, x_max, x_min, color=color_pred)\n",
    "\n",
    "  plt.vlines(rx0, ry0, ry1, color=color_right)\n",
    "  plt.vlines(rx1, ry1, ry0, color=color_right)\n",
    "  plt.hlines(ry0, rx0, rx1, color=color_right)\n",
    "  plt.hlines(ry1, rx1, rx0, color=color_right)\n",
    "\n",
    "  return coordinates\n",
    "\n",
    "pred_bboxes_s = cnn_pred(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jJg6zRI0y4KW"
   },
   "source": [
    "# Using the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0kpl4-5xy-3k"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import ultralytics\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "\n",
    "class Number_detection:\n",
    "  def __init__(self,\n",
    "               path_to_model,\n",
    "               yolo_version = \"yolov9c.pt\",\n",
    "               device = 'cpu'):\n",
    "    self.device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    self.detection_model = YOLO(yolo_version)class Number_detection:\n",
    "  def __init__(self,\n",
    "               path_to_model,\n",
    "               yolo_version = \"yolov9c.pt\",\n",
    "               device = 'cpu'):\n",
    "    self.device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    self.detection_model = YOLO(yolo_version)\n",
    "    self.cnn_model = torch.load(path_to_model).to(self.device)\n",
    "    self.cnn_model.train()\n",
    "\n",
    "  def cropping_image(self, image, x_min, y_min, x_max, y_max, x_shape, y_shape):\n",
    "    top = int(y_min * y_shape)\n",
    "    left = int(x_min * x_shape)\n",
    "    height = int((y_max - y_min) * y_shape)\n",
    "    width = int((x_max - x_min) * x_shape)\n",
    "    return transforms.functional.crop(img = image,\n",
    "                                      top = top,\n",
    "                                      left = left,\n",
    "                                      height = height,\n",
    "                                      width = width)\n",
    "\n",
    "  def detection_perform(self, image_path, min_size = 0.17):\n",
    "    detection_output = self.detection_model(image_path)[0]\n",
    "    classes_with_number = [2, 3, 5, 7]\n",
    "    transform = transforms.Resize((512, 512))\n",
    "    answer_images = torch.zeros((0, 3, 512, 512))\n",
    "    image_tensor = ST.jpg_tensor(image_path)\n",
    "    y_shape, x_shape = image_tensor.shape[-2:]\n",
    "    num_of_preds = 0\n",
    "\n",
    "    for current_class, (x_min, y_min, x_max, y_max) in zip(detection_output.boxes.cls, detection_output.boxes.xyxyn):\n",
    "      if current_class.item() in classes_with_number and x_max - x_min >= min_size and y_max - y_min >= min_size:\n",
    "        adding_image = transform(self.cropping_image(image = image_tensor,\n",
    "                                                     x_min = x_min,\n",
    "                                                     y_min = y_min,\n",
    "                                                     x_max = x_max,\n",
    "                                                     y_max = y_max,\n",
    "                                                     x_shape = x_shape,\n",
    "                                                     y_shape = y_shape)).unsqueeze(0)\n",
    "        answer_images = torch.cat((answer_images,\n",
    "                                   adding_image), dim=0)\n",
    "        num_of_preds += 1\n",
    "    return answer_images, num_of_preds\n",
    "\n",
    "  def plot_boxes(self, image, coordinates, color='red'):\n",
    "    x0, y0, x1, y1 = coordinates\n",
    "    x_min = min(x0, x1)\n",
    "    x_max = max(x0, x1)\n",
    "    y_min = min(y0, y1)\n",
    "    y_max = max(y0, y1)\n",
    "    ST.imshow(image)\n",
    "    plt.vlines(x_min, y_min, y_max, color=color)\n",
    "    plt.vlines(x_max, y_max, y_min, color=color)\n",
    "    plt.hlines(y_min, x_min, x_max, color=color)\n",
    "    plt.hlines(y_max, x_max, x_min, color=color)\n",
    "\n",
    "  def load_weights(self, path_to_weights):\n",
    "    self.cnn_model.load_state_dict(torch.load(path_to_weights))\n",
    "\n",
    "  def save_cnn_model(self, saving_path):\n",
    "    torch.save(self.cnn_model, saving_path)\n",
    "\n",
    "  def pred_perform(self, images_tensor, answer_bboxes):\n",
    "    answer_images = []\n",
    "    pil_transform = transforms.ToPILImage()\n",
    "    for current_image_tensor, (x_min, y_min, x_max, y_max) in zip(images_tensor, answer_bboxes):\n",
    "      cropped_image = self.cropping_image(image = current_image_tensor,\n",
    "                                          x_min = x_min,\n",
    "                                          y_min = y_min,\n",
    "                                          x_max = x_max,\n",
    "                                          y_max = y_max,\n",
    "                                          x_shape = 1,\n",
    "                                          y_shape = 1)\n",
    "      answer_images.append(pil_transform(cropped_image))\n",
    "    return answer_images\n",
    "\n",
    "  def IMAGE_PRED (self, image_path, show = True, color = 'red', min_size = 0.17):\n",
    "    images_tensor, num_of_preds = self.detection_perform(image_path, min_size=min_size)\n",
    "    answer_bboxes = (self.cnn_model((images_tensor.to(self.device)) % 1)).tolist()\n",
    "    received_images = self.pred_perform(images_tensor, answer_bboxes)\n",
    "    return images_tensor, answer_bboxes, received_images\n",
    "\n",
    "number_detector = Number_detection(\"drive/MyDrive/cnn.pt\")\n",
    "    self.cnn_model = torch.load(path_to_model).to(self.device)\n",
    "    self.cnn_model.train()\n",
    "\n",
    "  def cropping_image(self, image, x_min, y_min, x_max, y_max, x_shape, y_shape):\n",
    "    top = int(y_min * y_shape)\n",
    "    left = int(x_min * x_shape)\n",
    "    height = int((y_max - y_min) * y_shape)\n",
    "    width = int((x_max - x_min) * x_shape)\n",
    "    return transforms.functional.crop(img = image,\n",
    "                                      top = top,\n",
    "                                      left = left,\n",
    "                                      height = height,\n",
    "                                      width = width)\n",
    "\n",
    "  def detection_perform(self, image_path, min_size = 0.17):\n",
    "    detection_output = self.detection_model(image_path)[0]\n",
    "    classes_with_number = [2, 3, 5, 7]\n",
    "    transform = transforms.Resize((512, 512))\n",
    "    answer_images = torch.zeros((0, 3, 512, 512))\n",
    "    image_tensor = ST.jpg_tensor(image_path)\n",
    "    y_shape, x_shape = image_tensor.shape[-2:]\n",
    "    num_of_preds = 0\n",
    "\n",
    "    for current_class, (x_min, y_min, x_max, y_max) in zip(detection_output.boxes.cls, detection_output.boxes.xyxyn):\n",
    "      if current_class.item() in classes_with_number and x_max - x_min >= min_size and y_max - y_min >= min_size:\n",
    "        adding_image = transform(self.cropping_image(image = image_tensor,\n",
    "                                                     x_min = x_min,\n",
    "                                                     y_min = y_min,\n",
    "                                                     x_max = x_max,\n",
    "                                                     y_max = y_max,\n",
    "                                                     x_shape = x_shape,\n",
    "                                                     y_shape = y_shape)).unsqueeze(0)\n",
    "        answer_images = torch.cat((answer_images,\n",
    "                                   adding_image), dim=0)\n",
    "        num_of_preds += 1\n",
    "    return answer_images, num_of_preds\n",
    "\n",
    "  def plot_boxes(self, image, coordinates, color='red'):\n",
    "    x0, y0, x1, y1 = coordinates\n",
    "    x_min = min(x0, x1)\n",
    "    x_max = max(x0, x1)\n",
    "    y_min = min(y0, y1)\n",
    "    y_max = max(y0, y1)\n",
    "    ST.imshow(image)\n",
    "    plt.vlines(x_min, y_min, y_max, color=color)\n",
    "    plt.vlines(x_max, y_max, y_min, color=color)\n",
    "    plt.hlines(y_min, x_min, x_max, color=color)\n",
    "    plt.hlines(y_max, x_max, x_min, color=color)\n",
    "\n",
    "  def load_weights(self, path_to_weights):\n",
    "    self.cnn_model.load_state_dict(torch.load(path_to_weights))\n",
    "\n",
    "  def save_cnn_model(self, saving_path):\n",
    "    torch.save(self.cnn_model, saving_path)\n",
    "\n",
    "  def image_pred (self, image_path, show = True, color = 'red', min_size = 0.17):\n",
    "    images_tensor, num_of_preds = self.detection_perform(image_path, min_size=min_size)\n",
    "    answer_bboxes = (self.cnn_model((images_tensor.to(self.device)) % 1)).tolist()\n",
    "    return images_tensor, answer_bboxes\n",
    "\n",
    "number_detector = Number_detection(path_to_model=\"cnn.pt\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "YJI1iwI8UBWz",
    "sZ7MpyY1Wcwg",
    "hnGCgyu9XATN",
    "JxqkLuXr9MQT",
    "ZnDTrHSH9Cu8",
    "Va46AZA3-8Vt"
   ],
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
