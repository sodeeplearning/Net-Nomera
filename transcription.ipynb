{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Downloading and importing requeiered libraries"
      ],
      "metadata": {
        "id": "CZMY2dn1ihlR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import clear_output\n",
        "\n",
        "!git clone https://github.com/sodeeplearning/simpletorch\n",
        "!pip install ultralytics\n",
        "!pip install easyocr\n",
        "\n",
        "clear_output(True)\n",
        "print(\"All requiered modules have been downloaded\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tCYp0NnlimDv",
        "outputId": "16d8f3c5-5238-484f-96fd-9b10b3e707ab"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All requiered modules have been downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from ultralytics import YOLO\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import transforms\n",
        "import simpletorch as ST\n",
        "import cv2\n",
        "import easyocr\n",
        "import requests\n",
        "import io\n",
        "import zipfile\n",
        "import torchvision\n",
        "import PIL\n",
        "import os"
      ],
      "metadata": {
        "id": "64zCqL4bi_WC"
      },
      "execution_count": 228,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Downloading dataset"
      ],
      "metadata": {
        "id": "5NaCTsN7kJbp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Downloading has been started\")\n",
        "\n",
        "url = \"https://storage.yandexcloud.net/net-nomer-dataset/Net-Nomer-a-data_processing.zip\"\n",
        "\n",
        "response = requests.get(url)\n",
        "zip = zipfile.ZipFile(io.BytesIO(response.content))\n",
        "zip.extractall()\n",
        "del zip\n",
        "\n",
        "print(\"The dataset has been downloaded\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xG06RROPkLrO",
        "outputId": "3ea516e0-09f4-467f-d072-41735c9cd4d2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading has been started\n",
            "The dataset has been downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def string_perf(string):\n",
        "    return int(string[string.find('>') + 1: string.rfind('<')])\n",
        "\n",
        "\n",
        "def xml_perf(path):\n",
        "    with open(path, 'r') as f:\n",
        "        file = f.read()\n",
        "        massive = file.split('\\n')\n",
        "        return (string_perf(massive[-6]) / 1552 * 270,\n",
        "                string_perf(massive[-7]) / 2592 * 480,\n",
        "                string_perf(massive[-4]) / 1552 * 270,\n",
        "                string_perf(massive[-5]) / 2592 * 480)"
      ],
      "metadata": {
        "id": "6bdRgUwloVVm"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Getting models"
      ],
      "metadata": {
        "id": "8PuEuy9jjXWx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN_Model (torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    self.convolutions = torch.nn.Sequential(*[\n",
        "        ST.Conv_Block(\n",
        "            input_channels = 3,\n",
        "            output_channels = 128,\n",
        "        ),\n",
        "        ST.Conv_Block(\n",
        "            input_channels = 128,\n",
        "            output_channels = 128,\n",
        "        ),\n",
        "        ST.Conv_Block(\n",
        "            input_channels = 128,\n",
        "            output_channels = 256,\n",
        "        ),\n",
        "        ST.Conv_Block(\n",
        "            input_channels = 256,\n",
        "            output_channels = 512,\n",
        "        ),\n",
        "        ST.Conv_Block(\n",
        "            input_channels = 512,\n",
        "            output_channels = 512,\n",
        "        ),\n",
        "        ST.Conv_Block(\n",
        "            input_channels = 512,\n",
        "            output_channels = 512,\n",
        "        ),\n",
        "        torch.nn.Flatten()\n",
        "    ])\n",
        "\n",
        "    self.fs = torch.nn.Sequential(*[\n",
        "        torch.nn.Linear(32768, 100),\n",
        "        torch.nn.ReLU(),\n",
        "        torch.nn.Linear(100, 4),\n",
        "    ])\n",
        "\n",
        "  def forward(self, input_tensor):\n",
        "    feature_map = self.convolutions(input_tensor)\n",
        "    fs_bboxes = self.fs(feature_map)\n",
        "    return fs_bboxes"
      ],
      "metadata": {
        "id": "_xc3uP8bmc94"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Number_detection:\n",
        "  def __init__(self,\n",
        "               path_to_model,\n",
        "               yolo_version = \"yolov9c.pt\",\n",
        "               device = 'cpu'):\n",
        "    self.device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
        "    self.detection_model = YOLO(yolo_version)\n",
        "    self.cnn_model = torch.load(path_to_model).to(self.device)\n",
        "    self.cnn_model.train()\n",
        "\n",
        "  def cropping_image(self, image, x_min, y_min, x_max, y_max, x_shape, y_shape):\n",
        "    top = int(y_min * y_shape)\n",
        "    left = int(x_min * x_shape)\n",
        "    height = int((y_max - y_min) * y_shape)\n",
        "    width = int((x_max - x_min) * x_shape)\n",
        "    return transforms.functional.crop(img = image,\n",
        "                                      top = top,\n",
        "                                      left = left,\n",
        "                                      height = height,\n",
        "                                      width = width)\n",
        "\n",
        "  def detection_perform(self, image_path, min_size = 0.17):\n",
        "    detection_output = self.detection_model(image_path)[0]\n",
        "    classes_with_number = [2, 3, 5, 7]\n",
        "    transform = transforms.Resize((512, 512))\n",
        "    answer_images = torch.zeros((0, 3, 512, 512))\n",
        "    image_tensor = ST.jpg_tensor(image_path)\n",
        "    y_shape, x_shape = image_tensor.shape[-2:]\n",
        "    num_of_preds = 0\n",
        "\n",
        "    for current_class, (x_min, y_min, x_max, y_max) in zip(detection_output.boxes.cls, detection_output.boxes.xyxyn):\n",
        "      if current_class.item() in classes_with_number and x_max - x_min >= min_size and y_max - y_min >= min_size:\n",
        "        adding_image = transform(self.cropping_image(image = image_tensor,\n",
        "                                                     x_min = x_min,\n",
        "                                                     y_min = y_min,\n",
        "                                                     x_max = x_max,\n",
        "                                                     y_max = y_max,\n",
        "                                                     x_shape = x_shape,\n",
        "                                                     y_shape = y_shape)).unsqueeze(0)\n",
        "        answer_images = torch.cat((answer_images,\n",
        "                                   adding_image), dim=0)\n",
        "        num_of_preds += 1\n",
        "    return answer_images, num_of_preds\n",
        "\n",
        "  def plot_boxes(self, image, coordinates, color='red'):\n",
        "    x0, y0, x1, y1 = coordinates\n",
        "    x_min = min(x0, x1)\n",
        "    x_max = max(x0, x1)\n",
        "    y_min = min(y0, y1)\n",
        "    y_max = max(y0, y1)\n",
        "    ST.imshow(image)\n",
        "    plt.vlines(x_min, y_min, y_max, color=color)\n",
        "    plt.vlines(x_max, y_max, y_min, color=color)\n",
        "    plt.hlines(y_min, x_min, x_max, color=color)\n",
        "    plt.hlines(y_max, x_max, x_min, color=color)\n",
        "\n",
        "  def load_weights(self, path_to_weights):\n",
        "    self.cnn_model.load_state_dict(torch.load(path_to_weights))\n",
        "\n",
        "  def save_cnn_model(self, saving_path):\n",
        "    torch.save(self.cnn_model, saving_path)\n",
        "\n",
        "  def pred_perform(self, images_tensor, answer_bboxes):\n",
        "    answer_images = []\n",
        "    pil_transform = transforms.ToPILImage()\n",
        "    for current_image_tensor, (x_min, y_min, x_max, y_max) in zip(images_tensor, answer_bboxes):\n",
        "      cropped_image = self.cropping_image(image = current_image_tensor,\n",
        "                                          x_min = x_min,\n",
        "                                          y_min = y_min,\n",
        "                                          x_max = x_max,\n",
        "                                          y_max = y_max,\n",
        "                                          x_shape = 1,\n",
        "                                          y_shape = 1)\n",
        "      answer_images.append(pil_transform(cropped_image))\n",
        "    return answer_images\n",
        "\n",
        "  def IMAGE_PRED (self, image_path, show = True, color = 'red', min_size = 0.17):\n",
        "    images_tensor, num_of_preds = self.detection_perform(image_path, min_size=min_size)\n",
        "    answer_bboxes = (self.cnn_model((images_tensor.to(self.device)) % 1)).tolist()\n",
        "    received_images = self.pred_perform(images_tensor, answer_bboxes)\n",
        "    return images_tensor, answer_bboxes, received_images\n",
        "\n",
        "number_detector = Number_detection(\"drive/MyDrive/cnn.pt\")"
      ],
      "metadata": {
        "id": "RMocszChjOfg"
      },
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Number_recognizer:\n",
        "  def __init__(self,\n",
        "               detect_model : Number_detection,\n",
        "               languages = ['en'],\n",
        "               image_dir = 'images',\n",
        "               use_gpu = False,\n",
        "               use_detector = False):\n",
        "    self.text_reader = easyocr.Reader(lang_list = languages,\n",
        "                                      gpu = use_gpu,\n",
        "                                      detector = False)\n",
        "    self.detect_model = detect_model\n",
        "    self.images_dir = image_dir\n",
        "    self.characters = ['A', 'B', 'C', 'E', 'H', 'K', 'M', 'O', 'P', 'T', 'X', 'Y',\n",
        "              'a', 'b', 'c', 'e', 'h', 'k', 'm', 'o', 'p', 't', 'x', 'y',\n",
        "              \"а\", \"в\", \"с\", \"е\", \"н\", \"к\", \"м\", \"о\", \"р\", \"т\", \"х\", \"у\",\n",
        "              \"А\", \"В\", \"С\", \"Е\", \"Н\", \"К\", \"М\", \"О\", \"Р\", \"Т\", \"Х\", \"У\",\n",
        "              '1', '2', '3', '4', '5', '6', '7', '8', '9', '0']\n",
        "\n",
        "  def PRED(self, image_path):\n",
        "    images_tensor, bboxes, received_images = self.detect_model.IMAGE_PRED(image_path)\n",
        "    answer_output = []\n",
        "\n",
        "    for ind, current_image in enumerate(received_images):\n",
        "      current_image.save(os.path.join(self.images_dir, f\"image{ind}.jpg\"))\n",
        "\n",
        "    for ind, current_path in enumerate(ST.getting_files(self.images_dir)):\n",
        "      current_output = self.text_reader.recognize(current_path,\n",
        "                                                  allowlist = self.characters,\n",
        "                                                  detail = 0)\n",
        "      answer_output.append(current_output)\n",
        "\n",
        "    return images_tensor, bboxes, received_images, answer_output\n",
        "\n",
        "number_recognizer = Number_recognizer(detect_model = number_detector,\n",
        "                                      use_gpu = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gVAlpxu02934",
        "outputId": "579d9894-030e-40a7-f821-f0868798fbf5"
      },
      "execution_count": 229,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:easyocr.easyocr:Using CPU. Note: This module is much faster with a GPU.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test prediction"
      ],
      "metadata": {
        "id": "aiOOnsVoEiuK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "number_recognizer.PRED(\"Dataset/photo/0.jpg\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7DOlV5HRBv3M",
        "outputId": "0368a001-f754-4335-8722-fff72f6056e1"
      },
      "execution_count": 230,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "image 1/1 /content/Dataset/photo/0.jpg: 384x640 8 persons, 4 cars, 2 trucks, 2 backpacks, 1357.9ms\n",
            "Speed: 4.5ms preprocess, 1357.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[[[0.9956, 0.9987, 0.9986,  ..., 0.7244, 0.8961, 0.9919],\n",
              "           [0.9916, 0.9923, 0.9911,  ..., 0.7144, 0.8709, 0.9889],\n",
              "           [0.9340, 0.9318, 0.9143,  ..., 0.7126, 0.8424, 0.9814],\n",
              "           ...,\n",
              "           [0.9499, 0.9618, 0.9684,  ..., 0.4314, 0.4091, 0.3867],\n",
              "           [0.9530, 0.9670, 0.9724,  ..., 0.4256, 0.4093, 0.3999],\n",
              "           [0.9478, 0.9658, 0.9755,  ..., 0.4179, 0.4080, 0.4077]],\n",
              " \n",
              "          [[0.9956, 0.9987, 0.9986,  ..., 0.7375, 0.9074, 0.9891],\n",
              "           [0.9916, 0.9923, 0.9910,  ..., 0.7534, 0.8968, 0.9909],\n",
              "           [0.9340, 0.9317, 0.9124,  ..., 0.7672, 0.8806, 0.9902],\n",
              "           ...,\n",
              "           [0.9342, 0.9467, 0.9591,  ..., 0.3879, 0.3657, 0.3436],\n",
              "           [0.9373, 0.9519, 0.9631,  ..., 0.3764, 0.3608, 0.3560],\n",
              "           [0.9321, 0.9507, 0.9662,  ..., 0.3681, 0.3577, 0.3595]],\n",
              " \n",
              "          [[0.9956, 0.9987, 0.9986,  ..., 0.6546, 0.8343, 0.9086],\n",
              "           [0.9916, 0.9923, 0.9911,  ..., 0.6490, 0.8052, 0.8980],\n",
              "           [0.9299, 0.9276, 0.9084,  ..., 0.6448, 0.7724, 0.8766],\n",
              "           ...,\n",
              "           [0.9382, 0.9506, 0.9630,  ..., 0.3167, 0.2895, 0.2652],\n",
              "           [0.9413, 0.9558, 0.9670,  ..., 0.3024, 0.2852, 0.2780],\n",
              "           [0.9361, 0.9546, 0.9701,  ..., 0.2903, 0.2831, 0.2877]]]]),\n",
              " [[164.25094604492188,\n",
              "   374.27117919921875,\n",
              "   337.7810363769531,\n",
              "   433.12860107421875]],\n",
              " [<PIL.Image.Image image mode=RGB size=173x58>],\n",
              " [['KE']])"
            ]
          },
          "metadata": {},
          "execution_count": 230
        }
      ]
    }
  ]
}